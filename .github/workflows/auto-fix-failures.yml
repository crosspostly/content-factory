name: Auto-Fix Test Failures

on:
  workflow_run:
    workflows:
      - "Run Tests"
      - "1. Render Video Pipeline"
    types:
      - completed
    branches:
      - main
      - feature-*
      - project-*

jobs:
  auto-fix:
    runs-on: ubuntu-latest
    name: Gemini Auto-Fix (Retry Loop)
    if: github.event.workflow_run.conclusion == 'failure'
    timeout-minutes: 45
    permissions:
      actions: read
      contents: write
      pull-requests: write
      issues: write

    env:
      GOOGLE_AI_API_KEY: ${{ secrets.GOOGLE_AI_API_KEY }}
      PIXABAY_API_KEY: ${{ secrets.PIXABAY_API_KEY }}

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}
          ref: ${{ github.event.workflow_run.head_branch }}

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: 'requirements.txt'

      - name: Clear pip cache completely
        run: |
          echo "=== CLEARING PIP CACHE ==="
          pip cache purge
          rm -rf ~/.cache/pip
          echo "‚úÖ Cache cleared"
          echo ""

      - name: Install Dependencies (NO CACHE)
        run: |
          python -m pip install --upgrade pip --no-cache-dir
          pip install -q google-generativeai -r requirements.txt --no-cache-dir --force-reinstall
          echo "‚úÖ Dependencies installed"
          echo ""

      - name: Download Test Logs
        uses: actions/download-artifact@v4
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          run-id: ${{ github.event.workflow_run.id }}
          path: workflow-artifacts
        continue-on-error: true

      - name: Auto-Fix Loop (Up to 10 Attempts)
        continue-on-error: true
        run: |
          python3 << 'SCRIPT_EOF'
          import json
          import os
          import re
          import subprocess
          import sys
          from pathlib import Path

          import google.generativeai as genai

          # Validate and get API key
          api_key = os.getenv('GOOGLE_AI_API_KEY', '').strip()
          
          if not api_key:
              print("‚ùå ERROR: GOOGLE_AI_API_KEY is not configured!")
              print("Please add GOOGLE_AI_API_KEY to Settings ‚Üí Secrets and variables ‚Üí Actions")
              with open('FIX_SUCCESS', 'w', encoding='utf-8') as f:
                  f.write('false')
              sys.exit(0)
          
          print("‚úÖ API Key is configured")
          genai.configure(api_key=api_key)

          ALLOWED_MODELS = ['gemini-2.5-flash', 'gemini-2.0-flash', 'gemini-1.5-pro']
          CURRENT_MODEL = 'gemini-2.5-flash'

          MAX_ATTEMPTS = 10
          SUCCESS = False
          WORKFLOW_NAME = os.getenv('WORKFLOW_NAME', 'Unknown')

          for attempt in range(1, MAX_ATTEMPTS + 1):
              print(f"\n{'='*60}")
              print(f"üîß ATTEMPT {attempt}/{MAX_ATTEMPTS}")
              print(f"Model: {CURRENT_MODEL}")
              print(f"Workflow: {WORKFLOW_NAME}")
              print(f"{'='*60}\n")

              # Step 1: Extract errors from logs
              print("üîç Extracting error information...")
              error_info = []
              failed_tests = []
              error_messages = []
              import_errors = []

              # Scan all log files
              for log_file in Path('workflow-artifacts').rglob('*'):
                  if log_file.is_file():
                      try:
                          with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                              content = f.read()
                              
                              # Test failures
                              for match in re.finditer(r'FAILED (.*?) -', content):
                                  failed_tests.append(match.group(1))
                              
                              # General errors
                              for match in re.finditer(
                                  r'(AssertionError|ValueError|TypeError|AttributeError|ImportError|ModuleNotFoundError|Exception)[:=\s]+(.*?)(?=\n|$)',
                                  content,
                              ):
                                  error_messages.append(f"{match.group(1)}: {match.group(2)[:200]}")
                              
                              # Module not found errors (critical for render_video)
                              for match in re.finditer(r'ModuleNotFoundError: No module named \'([^\']+)\'', content):
                                  import_errors.append(match.group(1))
                              
                              # Tracebacks
                              traceback_matches = re.findall(
                                  r'(Traceback.*?)(?=\n[A-Z]|\nFAILED|$)',
                                  content,
                                  re.DOTALL,
                              )
                              for tb in traceback_matches[:3]:
                                  error_info.append(tb[:500])
                      except Exception:
                          pass

              # Also check pytest log if it exists
              if Path('pytest-last.log').exists():
                  with open('pytest-last.log', 'r', encoding='utf-8', errors='ignore') as f:
                      content = f.read()
                      for match in re.finditer(r'FAILED (.*?) -', content):
                          failed_tests.append(match.group(1))
                      for match in re.finditer(
                          r'(AssertionError|ValueError|TypeError)[:=\s]+(.*?)(?=\n|$)',
                          content,
                      ):
                          error_messages.append(f"{match.group(1)}: {match.group(2)[:200]}")

              # Deduplicate
              failed_tests = list(set(failed_tests))
              error_messages = list(set(error_messages))
              import_errors = list(set(import_errors))

              if not failed_tests and not error_messages and not import_errors:
                  print("‚ö†Ô∏è No errors found in logs")
                  break

              print(f"‚ùå Found {len(failed_tests)} failed tests")
              print(f"‚ùå Found {len(error_messages)} error messages")
              print(f"‚ùå Found {len(import_errors)} missing modules: {import_errors}")

              # Step 2: Collect code context
              code_files = []
              for py_file in Path('core').rglob('*.py'):
                  if '__pycache__' not in str(py_file):
                      try:
                          with open(py_file, 'r', encoding='utf-8', errors='ignore') as f:
                              code_files.append({'path': str(py_file), 'content': f.read()[:2000]})
                      except Exception:
                          pass

              # Also include requirements.txt for analysis
              req_content = ""
              if Path('requirements.txt').exists():
                  with open('requirements.txt', 'r', encoding='utf-8') as f:
                      req_content = f.read()

              # Step 3: Call Gemini for analysis
              print(f"ü§ñ Calling Gemini for analysis ({CURRENT_MODEL})...")

              prompt = f"""You are debugging Python failures in a GitHub Actions workflow. Attempt {attempt}/{MAX_ATTEMPTS}.

**Workflow:** {WORKFLOW_NAME}

**Failed Tests:**
{chr(10).join(failed_tests[:10]) if failed_tests else 'None'}

**Error Messages:**
{chr(10).join(error_messages[:10]) if error_messages else 'None'}

**Missing Modules (ImportError/ModuleNotFoundError):**
{chr(10).join([f"- {m}" for m in import_errors]) if import_errors else 'None'}

**Tracebacks:**
{chr(10).join(error_info[:2]) if error_info else 'None'}

**Current requirements.txt:**
```
{req_content}
```

**Code Context (sample files):**
{json.dumps(code_files[:3], indent=2)}

**Task:**
1. Identify the EXACT root cause
2. If it's a missing module error, ADD the missing package to requirements.txt
3. If it's a code error, provide PRECISE code fixes
4. Show complete file paths
5. Include exact line replacements

**Format for requirements.txt changes:**
```
FILE: requirements.txt
BEFORE:
```python
[current content]
```
AFTER:
```python
[fixed content with missing packages added]
```
REASON: [why this fixes it]
```

**Format for code changes:**
```
FILE: path/to/file.py
BEFORE:
```python
[exact code to replace]
```
AFTER:
```python
[fixed code]
```
REASON: [why this fixes it]
```

Be aggressive. Fix everything you can identify. If imports are missing, ADD THEM TO requirements.txt."""

              try:
                  if CURRENT_MODEL not in ALLOWED_MODELS:
                      raise ValueError(f"Model not allowed: {CURRENT_MODEL}")

                  model = genai.GenerativeModel(CURRENT_MODEL)
                  response = model.generate_content(prompt)

                  with open(f'gemini_analysis_attempt_{attempt}.md', 'w', encoding='utf-8') as f:
                      f.write(f"# Attempt {attempt}\n\n{response.text}")

                  print("‚úÖ Analysis complete\n")

                  # Step 4: Apply fixes
                  print("üîß Applying fixes...")

                  fix_pattern = (
                      r"FILE:\s*(.+?)\s*BEFORE:\s*```(?:python)?\s*(.+?)```\s*AFTER:\s*```(?:python)?\s*(.+?)```"
                  )
                  fixes_applied = 0

                  for match in re.finditer(fix_pattern, response.text, re.DOTALL | re.IGNORECASE):
                      filepath = match.group(1).strip()
                      before = match.group(2).strip()
                      after = match.group(3).strip()

                      if Path(filepath).exists():
                          try:
                              with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                                  file_content = f.read()

                              if before in file_content:
                                  new_content = file_content.replace(before, after, 1)
                                  with open(filepath, 'w', encoding='utf-8') as f:
                                      f.write(new_content)
                                  fixes_applied += 1
                                  print(f"  ‚úÖ Fixed {filepath}")
                              else:
                                  print(f"  ‚ö†Ô∏è Pattern not found in {filepath}")
                          except Exception as e:
                              print(f"  ‚ùå Error fixing {filepath}: {e}")
                      else:
                          print(f"  ‚ö†Ô∏è File not found: {filepath}")

                  print(f"\nüì¶ Applied {fixes_applied} fixes\n")

              except Exception as e:
                  print(f"‚ùå Gemini error: {e}")
                  continue

              # Step 5: Run appropriate tests
              print("üß™ Running tests/validation...")
              try:
                  if WORKFLOW_NAME and "Render Video" in WORKFLOW_NAME:
                      # For render_video workflow, just verify imports
                      print("   Testing imports...")
                      result = subprocess.run(
                          ['python', '-c', 'import requests; import moviepy; import PIL; import google.generativeai; print("‚úÖ All imports successful")'],
                          capture_output=True,
                          text=True,
                          timeout=30,
                      )
                      success = result.returncode == 0
                      test_output = result.stdout + result.stderr
                  else:
                      # For pytest workflows
                      result = subprocess.run(
                          ['pytest', 'tests/', '-v', '--tb=short', '-m', 'not slow', '--maxfail=5'],
                          capture_output=True,
                          text=True,
                          timeout=300,
                      )
                      success = result.returncode == 0
                      test_output = result.stdout + result.stderr

                  with open('pytest-last.log', 'w', encoding='utf-8') as f:
                      f.write(test_output)

                  if success:
                      print("‚úÖ TESTS/IMPORTS PASSED!")
                      SUCCESS = True
                      break

                  print(f"‚ùå Tests still failing")

              except subprocess.TimeoutExpired:
                  print("‚ö†Ô∏è Tests timeout")
              except Exception as e:
                  print(f"‚ùå Test run error: {e}")

          # Result
          print(f"\n{'='*60}")
          if SUCCESS:
              print("üéâ AUTO-FIX SUCCESSFUL!")
              with open('FIX_SUCCESS', 'w', encoding='utf-8') as f:
                  f.write('true')
          else:
              print(f"üî¥ FAILED AFTER {MAX_ATTEMPTS} ATTEMPTS")
              with open('FIX_SUCCESS', 'w', encoding='utf-8') as f:
                  f.write('false')
          print(f"{'='*60}\n")
          SCRIPT_EOF
        env:
          WORKFLOW_NAME: ${{ github.event.workflow_run.name }}

      - name: Check Fix Status
        id: fix-status
        run: |
          if [ -f "FIX_SUCCESS" ] && [ "$(cat FIX_SUCCESS)" = "true" ]; then
            echo "success=true" >> $GITHUB_OUTPUT
          else
            echo "success=false" >> $GITHUB_OUTPUT
          fi

      - name: Create Fix Branch & PR
        if: steps.fix-status.outputs.success == 'true'
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            const { execSync } = require('child_process');

            try {
              execSync('git config user.email "github-actions[bot]@github.com"');
              execSync('git config user.name "github-actions[bot]"');

              const branchName = `gemini-auto-fix-${context.runId}`;

              execSync(`git checkout -b ${branchName}`);
              execSync('git add -A');

              const status = execSync('git status --porcelain').toString().trim();
              if (!status) {
                console.log('‚ö†Ô∏è No changes detected after auto-fix; skipping PR creation.');
                return;
              }

              execSync('git commit -m "ü§ñ Auto-fix: resolve failing tests and missing imports\n\nApplied fixes after iterative analysis and testing."');
              execSync(`git push origin ${branchName}`);

              let allAnalysis = '';
              for (let i = 1; i <= 10; i++) {
                const file = `gemini_analysis_attempt_${i}.md`;
                if (fs.existsSync(file)) {
                  allAnalysis += fs.readFileSync(file, 'utf8') + '\n\n---\n\n';
                }
              }

              const pr = await github.rest.pulls.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: '‚úÖ [AUTO-FIX] Tests and imports fixed by Gemini',
                head: branchName,
                base: '${{ github.event.workflow_run.head_branch }}',
                body: `## üéâ Automated Fix Success\n\nGemini successfully analyzed and fixed the failing tests/imports through iterative debugging.\n\n### Analysis History\n${allAnalysis.slice(0, 5000)}\n\n---\n\n**Original Run:** https://github.com/${{ github.repository }}/actions/runs/${{ github.event.workflow_run.id }}\n**Fixed by:** Auto-Fix Workflow`
              });

              console.log(`‚úÖ PR Created: #${pr.data.number}`);

              try {
                await github.rest.issues.addLabels({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: pr.data.number,
                  labels: ['auto-fix', 'ai-generated']
                });
              } catch (e) {
                console.log(`‚ö†Ô∏è Could not add PR labels: ${e.message}`);
              }

              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: pr.data.number,
                body: 'üöÄ Ready to merge! All tests and imports are now working.'
              });
            } catch (error) {
              console.log(`‚ùå Error: ${error.message}`);
            }

      - name: Create Frozen Issue (Auto-Fix Only)
        if: steps.fix-status.outputs.success == 'false'
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');

            try {
              let allAnalysis = '';
              for (let i = 1; i <= 10; i++) {
                const file = `gemini_analysis_attempt_${i}.md`;
                if (fs.existsSync(file)) {
                  allAnalysis += `## Attempt ${i}\n${fs.readFileSync(file, 'utf8')}\n\n`;
                }
              }

              const actorLogin = context.payload?.workflow_run?.actor?.login;

              const issue = await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: 'üßä [AUTO-FIX FAILED] Test failures - manual intervention required',
                body: `## üî¥ Auto-Fix Failed\n\n**This issue was created automatically by the auto-fix workflow.**\n\nAfter 10 iterative attempts, the workflow could not automatically resolve the failures.\n\n### Attempts\n${allAnalysis.slice(0, 10000)}\n\n---\n\n### Manual action required\nReview the analysis above and fix manually.\n\n**Original Run:** https://github.com/${{ github.repository }}/actions/runs/${{ github.event.workflow_run.id }}\n**Workflow:** ${{ github.event.workflow_run.name }}\n**Branch:** ${{ github.event.workflow_run.head_branch }}`
              });

              console.log(`üìù Issue created: #${issue.data.number}`);

              try {
                await github.rest.issues.addLabels({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: issue.data.number,
                  labels: ['bug', 'auto-fix-failed']
                });
              } catch (e) {
                console.log(`‚ö†Ô∏è Could not add issue labels: ${e.message}`);
              }

              if (actorLogin) {
                try {
                  await github.rest.issues.addAssignees({
                    owner: context.repo.owner,
                    repo: context.repo.repo,
                    issue_number: issue.data.number,
                    assignees: [actorLogin]
                  });
                } catch (e) {
                  console.log(`‚ö†Ô∏è Could not assign issue to ${actorLogin}: ${e.message}`);
                }
              }
            } catch (error) {
              console.log(`‚ùå Failed to create issue: ${error.message}`);
            }

      - name: Upload Analysis Artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: gemini-auto-fix-attempts-${{ github.run_number }}
          path: |
            gemini_analysis_attempt_*.md
            pytest-last.log
          retention-days: 14
